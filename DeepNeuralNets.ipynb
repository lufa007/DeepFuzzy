{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import tensorflow.keras.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers:  [784, 256, 64, 10]\n",
      "Learning Rate:  0.01\n",
      "Number of MiniBatch:  128\n",
      "Iterations:  10000\n"
     ]
    }
   ],
   "source": [
    "#Parameters - To be defined by user\n",
    "nClass = 10\n",
    "nHidden = [256, 64]\n",
    "nInput = 784\n",
    "layers = [nInput] + nHidden + [nClass]\n",
    "limit = 0.0001\n",
    "#Hyperparameters - To be tuned by the user\n",
    "learning_rate = 0.01\n",
    "nMiniBatch = 128\n",
    "nIter = 10000\n",
    "print(\"Layers: \", layers)\n",
    "print(\"Learning Rate: \", learning_rate)\n",
    "print(\"Number of MiniBatch: \", nMiniBatch)\n",
    "print(\"Iterations: \", nIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------data input function-----------------------------#\n",
    "def getDataset(name, nClass):\n",
    "    if name==\"mnist\":\n",
    "        dataset = datasets.mnist\n",
    "    (x_train, y_train),(x_test, y_test) = dataset.load_data()     #downloading and loading the dataset\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0             #normalizing the input data\n",
    "    x_train_flat = x_train.reshape(x_train.shape[0],-1).T         #making dataset suitable for input in Fully Connected layer\n",
    "    x_test_flat = x_test.reshape(x_test.shape[0],-1).T            #making dataset suitable for input in Fully Connected layer\n",
    "    y_train_onehot = np.eye(nClass)[y_train].T                    #converting to one hot vectors\n",
    "    y_test_onehot = np.eye(nClass)[y_test].T                      #converting to one hot vectors\n",
    "    return x_train_flat,x_test_flat,y_train_onehot,y_test_onehot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deepfuzzy:\n",
    "    W = []\n",
    "    b = []\n",
    "    parameters = dict()\n",
    "    act = []\n",
    "    def __init__(self, layers, x_train, x_test, y_train, y_test, minibatch_size, learning_rate=0.01, iterations=100):\n",
    "        self.layers = layers\n",
    "        self.x_train_batch = x_train[:, :minibatch_size]\n",
    "        self.x_test = x_test\n",
    "        self.y_train_batch = y_train[:, :minibatch_size]\n",
    "        self.y_test = y_test\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.batch = minibatch_size\n",
    "        self.iter = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0,x)\n",
    "    @staticmethod\n",
    "    def linear_activation_backward(dA,cache,activation):\n",
    "        if(activation==\"sigmoid\"):\n",
    "            act =  cache \n",
    "            return np.multiply(np.multiply(dA, act), 1-act)\n",
    "        if(activation==\"relu\"):\n",
    "            act = cache\n",
    "            act[act>0] = 1\n",
    "            act[act<0] = 0\n",
    "            return np.multiply(dA, act)\n",
    "    def initialize(self, initializer = 'random'):\n",
    "        if initializer == 'random':\n",
    "            for i in range(len(self.layers)-1):\n",
    "                self.W.append(np.random.rand(self.layers[i+1],self.layers[i])*0.02)\n",
    "                self.b.append(np.random.rand(self.layers[i+1],1))\n",
    "                assert(self.W[i].shape == (self.layers[i+1], self.layers[i]))\n",
    "                assert(self.b[i].shape == (self.layers[i+1], 1))\n",
    "        elif initializer == 'xavier':\n",
    "            pass\n",
    "        self.parameters['W'] = self.W\n",
    "        self.parameters['b'] = self.b\n",
    "    def forwardProp(self):\n",
    "        self.act=[]\n",
    "        self.act.append(self.x_train_batch)\n",
    "        for i in range(len(self.layers)-2):\n",
    "            z = np.dot(self.parameters['W'][i], self.act[-1])\n",
    "            self.act.append(deepfuzzy.relu(z + self.parameters['b'][i]))  #relu\n",
    "        self.act.append(deepfuzzy.softmax(np.dot(self.parameters['W'][len(self.layers)-2], self.act[-1])))\n",
    "    def compCost(self):\n",
    "        interm = np.dot(np.log(self.act[-1]).T,self.y_train_batch)\n",
    "        cost = -1.0/self.y_train_batch.shape[1]*np.sum(np.trace(interm))\n",
    "        return cost\n",
    "    def backProp(self):\n",
    "        m = self.y_train_batch.shape[1]\n",
    "        dZ = self.act[-1] - self.y_train_batch\n",
    "        dW = 1.0/m * np.dot(dZ, self.act[-2].T)\n",
    "        db = 1.0/m * np.dot(dZ, self.act[-2].T)\n",
    "        dA_prev = np.dot(self.parameters['W'][-1].T, dZ)\n",
    "        self.parameters['W'][-1] = self.parameters['W'][-1] - self.learning_rate*dW\n",
    "        self.parameters['b'][-1] = self.parameters['b'][-1] - self.learning_rate*db\n",
    "        for i in reversed(range(len(self.layers)-2)):\n",
    "            dA = dA_prev \n",
    "            dZ = deepfuzzy.linear_activation_backward(dA,self.act[i+1],\"relu\")\n",
    "            dW = 1.0/m * np.dot(dZ, self.act[i].T)\n",
    "            db = 1.0/m*  np.sum(np.array(dZ),axis=1,keepdims=True)\n",
    "            dA_prev = np.dot(self.parameters['W'][i].T, dZ)\n",
    "            self.parameters['W'][i] = self.parameters['W'][i] - self.learning_rate*dW\n",
    "            self.parameters['b'][i] = self.parameters['b'][i] - self.learning_rate*db\n",
    "    def check_accuracy(self, y, x):\n",
    "        mat = np.zeros([10,10])\n",
    "        m = y.shape[1]\n",
    "        buff_x = self.x_train_batch\n",
    "        self.x_train_batch = x[:,]\n",
    "        self.forwardProp()\n",
    "        pred = np.argmax(self.act[-1], axis = 0)\n",
    "        exp = np.argmax(y, axis = 0)\n",
    "        error = np.sum(exp!=pred)\n",
    "        self.x_train_batch = buff_x\n",
    "        for i in range(m):\n",
    "            mat[exp[i]][pred[i]] =  mat[exp[i]][pred[i]] + 1\n",
    "        # Calculate accuracy\n",
    "        return (m - error)/m * 100, mat\n",
    "    def train(self):\n",
    "        for i in range(self.iter):\n",
    "                if(i%2==0):\n",
    "                    idx = np.random.randint(self.x_train.shape[1], size=self.batch)\n",
    "                    self.x_train_batch = self.x_train[:,idx]\n",
    "                    self.y_train_batch = self.y_train[:,idx]\n",
    "                self.forwardProp()\n",
    "                cost = self.compCost()\n",
    "                self.backProp()\n",
    "                if(i%100 == 0):\n",
    "                    Accuracy, _ = self.check_accuracy(self.y_train,self.x_train)\n",
    "                    print(\"Accuracy: \", Accuracy)\n",
    "                    self.saveWeights()\n",
    "    def test(self):\n",
    "        self.forwardProp()\n",
    "        Accuracy, mat = self.check_accuracy(self.y_test,self.x_test)\n",
    "        print(\"Test Accuracy\", Accuracy )\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(mat)\n",
    "    def saveWeights(self):\n",
    "        np.save('W',self.parameters['W'])\n",
    "        np.save('b',self.parameters['b'])\n",
    "    def loadWeights(self):\n",
    "        try:\n",
    "            W = np.load('W.npy')\n",
    "            b = np.load('b.npy')\n",
    "            self.parameters['W'] = W\n",
    "            self.parameters['b'] = b\n",
    "        except:\n",
    "            print(\"Not able to load weights!\")\n",
    "            print(\"Initializing Weights...\")\n",
    "            self.initialize()\n",
    "            print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    x_train, x_test, y_train, y_test = getDataset(\"mnist\", nClass)\n",
    "    test = deepfuzzy(layers, x_train, x_test, y_train, y_test, nMiniBatch, learning_rate=0.01, iterations=10000)\n",
    "    test.initialize()\n",
    "    test.train()\n",
    "    test.test()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  9.87166666667\n",
      "Accuracy:  9.87166666667\n",
      "Accuracy:  9.75166666667\n",
      "Accuracy:  18.71\n",
      "Accuracy:  14.1633333333\n",
      "Accuracy:  19.965\n",
      "Accuracy:  22.26\n",
      "Accuracy:  22.55\n",
      "Accuracy:  23.925\n",
      "Accuracy:  27.2366666667\n",
      "Accuracy:  28.6183333333\n",
      "Accuracy:  31.16\n",
      "Accuracy:  32.1783333333\n",
      "Accuracy:  32.15\n",
      "Accuracy:  33.4183333333\n",
      "Accuracy:  32.8966666667\n",
      "Accuracy:  34.44\n",
      "Accuracy:  35.835\n",
      "Accuracy:  35.1866666667\n",
      "Accuracy:  36.1633333333\n",
      "Accuracy:  36.9916666667\n",
      "Accuracy:  36.5633333333\n",
      "Accuracy:  36.0383333333\n",
      "Accuracy:  36.6316666667\n",
      "Accuracy:  37.69\n",
      "Accuracy:  38.8433333333\n",
      "Accuracy:  39.1666666667\n",
      "Accuracy:  36.8666666667\n",
      "Accuracy:  38.94\n",
      "Accuracy:  37.3183333333\n",
      "Accuracy:  41.0733333333\n",
      "Accuracy:  40.3533333333\n",
      "Accuracy:  41.1733333333\n",
      "Accuracy:  42.0816666667\n",
      "Accuracy:  45.6033333333\n",
      "Accuracy:  48.3583333333\n",
      "Accuracy:  48.5916666667\n",
      "Accuracy:  52.985\n",
      "Accuracy:  57.0083333333\n",
      "Accuracy:  60.4283333333\n",
      "Accuracy:  59.8316666667\n",
      "Accuracy:  62.2583333333\n",
      "Accuracy:  63.885\n",
      "Accuracy:  65.575\n",
      "Accuracy:  66.4316666667\n",
      "Accuracy:  66.0633333333\n",
      "Accuracy:  66.2883333333\n",
      "Accuracy:  68.135\n",
      "Accuracy:  68.3966666667\n",
      "Accuracy:  70.2333333333\n",
      "Accuracy:  71.4816666667\n",
      "Accuracy:  73.3966666667\n",
      "Accuracy:  75.3\n",
      "Accuracy:  75.595\n",
      "Accuracy:  77.3866666667\n",
      "Accuracy:  78.19\n",
      "Accuracy:  78.1716666667\n",
      "Accuracy:  78.8733333333\n",
      "Accuracy:  80.0966666667\n",
      "Accuracy:  80.225\n",
      "Accuracy:  80.8883333333\n",
      "Accuracy:  81.6283333333\n",
      "Accuracy:  82.16\n",
      "Accuracy:  82.6266666667\n",
      "Accuracy:  82.8033333333\n",
      "Accuracy:  83.4816666667\n",
      "Accuracy:  83.9183333333\n",
      "Accuracy:  84.3433333333\n",
      "Accuracy:  84.795\n",
      "Accuracy:  85.055\n",
      "Accuracy:  85.485\n",
      "Accuracy:  85.8716666667\n",
      "Accuracy:  86.005\n",
      "Accuracy:  85.8183333333\n",
      "Accuracy:  86.3733333333\n",
      "Accuracy:  86.0616666667\n",
      "Accuracy:  86.0133333333\n",
      "Accuracy:  86.6233333333\n",
      "Accuracy:  86.7916666667\n",
      "Accuracy:  87.1383333333\n",
      "Accuracy:  86.9716666667\n",
      "Accuracy:  87.33\n",
      "Accuracy:  87.4816666667\n",
      "Accuracy:  87.47\n",
      "Accuracy:  87.4866666667\n",
      "Accuracy:  87.4283333333\n",
      "Accuracy:  87.6533333333\n",
      "Accuracy:  87.6733333333\n",
      "Accuracy:  87.6933333333\n",
      "Accuracy:  87.9216666667\n",
      "Accuracy:  87.75\n",
      "Accuracy:  87.9483333333\n",
      "Accuracy:  88.2\n",
      "Accuracy:  87.91\n",
      "Accuracy:  87.82\n",
      "Accuracy:  88.4383333333\n",
      "Accuracy:  88.2316666667\n",
      "Accuracy:  88.515\n",
      "Accuracy:  88.505\n",
      "Accuracy:  88.7966666667\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-93acd10641d2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b3d1c49210d2>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mAccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAccuracy\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Confusion Matrix:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
