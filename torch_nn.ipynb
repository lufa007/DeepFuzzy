{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tensorflow.keras.datasets as datasets\n",
    "from torch_dataload import MyDataset\n",
    "import nn_fuzzy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers:  [784, 256, 10]\n",
      "Learning Rate:  0.01\n",
      "Number of MiniBatch:  128\n",
      "Iterations:  10000\n",
      "Layers:  [2352, 256, 10]\n",
      "Learning Rate:  0.01\n",
      "Number of MiniBatch:  128\n",
      "Iterations:  10000\n",
      "Layers:  [39, 8, 2]\n",
      "Learning Rate:  0.01\n",
      "Number of MiniBatch:  16\n",
      "Iterations:  10000\n"
     ]
    }
   ],
   "source": [
    "#Parameters - To be defined by user\n",
    "nClass = 10\n",
    "nHidden = [256]\n",
    "nInput = 784\n",
    "layers = [nInput] + nHidden + [nClass]\n",
    "limit = 0.0001\n",
    "#Hyperparameters - To be tuned by the user\n",
    "learning_rate = 0.01\n",
    "nMiniBatch = 128\n",
    "nIter = 10000\n",
    "print(\"Layers: \", layers)\n",
    "print(\"Learning Rate: \", learning_rate)\n",
    "print(\"Number of MiniBatch: \", nMiniBatch)\n",
    "print(\"Iterations: \", nIter)\n",
    "\n",
    "#Parameters (for fuzzy)- To be defined by user\n",
    "nClass_fuzzy = 10\n",
    "nHidden_fuzzy = [256]\n",
    "nInput_fuzzy = 784 * 3\n",
    "layers_fuzzy = [nInput_fuzzy] + nHidden_fuzzy + [nClass_fuzzy]\n",
    "limit = 0.0001\n",
    "#Hyperparameters - To be tuned by the user\n",
    "learning_rate = 0.01\n",
    "nMiniBatch = 128\n",
    "nIter = 10000\n",
    "print(\"Layers: \", layers_fuzzy)\n",
    "print(\"Learning Rate: \", learning_rate)\n",
    "print(\"Number of MiniBatch: \", nMiniBatch)\n",
    "print(\"Iterations: \", nIter)\n",
    "\n",
    "#Parameters (for heart)- To be defined by user\n",
    "nClass_heart = 2\n",
    "nHidden_heart = [8]\n",
    "nInput_heart = 13 * 3\n",
    "layers_heart = [nInput_heart] + nHidden_heart + [nClass_heart]\n",
    "limit = 0.0001\n",
    "#Hyperparameters - To be tuned by the user\n",
    "learning_rate = 0.01\n",
    "nMiniBatch = 16\n",
    "nIter = 10000\n",
    "print(\"Layers: \", layers_heart)\n",
    "print(\"Learning Rate: \", learning_rate)\n",
    "print(\"Number of MiniBatch: \", nMiniBatch)\n",
    "print(\"Iterations: \", nIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(129, 784)\n"
     ]
    }
   ],
   "source": [
    "#-------data input function-----------------------------#\n",
    "def getDataset(name, nClass):\n",
    "    if name==\"mnist\":\n",
    "        dataset = datasets.mnist\n",
    "    (x_train, y_train),(x_test, y_test) = dataset.load_data()     #downloading and loading the dataset\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0             #normalizing the input data\n",
    "    x_train_flat = x_train.reshape(x_train.shape[0],-1)         #making dataset suitable for input in Fully Connected layer\n",
    "    x_test_flat = x_test.reshape(x_test.shape[0],-1)          #making dataset suitable for input in Fully Connected layer\n",
    "    y_train_onehot = np.eye(nClass)[y_train]                    #converting to one hot vectors\n",
    "    y_test_onehot = np.eye(nClass)[y_test]                     #converting to one hot vectors\n",
    "    print(x_train_flat.shape)\n",
    "    print(y_train_onehot.shape)\n",
    "    x_train_batch = np.array_split(x_train_flat, int(60000/128))\n",
    "    print(x_train_batch[2].shape)\n",
    "    return x_train_flat,x_test_flat,y_train_onehot,y_test_onehot\n",
    "\n",
    "x_train, x_test, y_train, y_test = getDataset(\"mnist\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fuzzify - x_train : 145.63284945487976\n",
      "Time taken to fuzzify - x_test : 21.033327102661133\n",
      "(10, 784)\n",
      "(10, 784)\n",
      "Time taken to fuzzify - y_train : 7.164988040924072\n",
      "Time taken to fuzzify - y_train : 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "x_train_fuzzy, x_test_fuzzy, y_train_fuzzy, y_test_fuzzy = nn_fuzzy.fuzzify_dataset(x_train, x_test, y_train, y_test, cnn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_fuzzy = x_train_fuzzy.reshape(60000, 3, 28, 28)\n",
    "# x_test_fuzzy = x_test_fuzzy.reshape(10000, 3, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa1b00786b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.25418435e-01, 3.16540276e-04, 3.24934009e-02, 2.34406898e-02,\n",
       "       1.59187933e-02, 3.22759307e-02, 1.30781527e-02, 2.49335401e-07,\n",
       "       3.02577342e-02, 6.42767310e-03])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[1].reshape(28,28), cmap='Greys')\n",
    "y_train_fuzzy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset = MyDataset(x_train, x_test, y_train, y_test, train=True)\n",
    "trainset = MyDataset(x_train, x_test, y_train, y_test, train=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# testset = MyDataset(x_train, x_test, y_train, y_test, train=False)\n",
    "testset = MyDataset(x_train, x_test, y_train, y_test, train=False)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 1, 28, 28)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.001\n",
      "[1,    40] loss: 0.001\n",
      "[1,    60] loss: 0.001\n",
      "[1,    80] loss: 0.001\n",
      "[1,   100] loss: 0.001\n",
      "[1,   120] loss: 0.001\n",
      "[1,   140] loss: 0.001\n",
      "[1,   160] loss: 0.001\n",
      "[1,   180] loss: 0.001\n",
      "[1,   200] loss: 0.001\n",
      "[1,   220] loss: 0.001\n",
      "[1,   240] loss: 0.001\n",
      "[1,   260] loss: 0.001\n",
      "[1,   280] loss: 0.001\n",
      "[1,   300] loss: 0.001\n",
      "[1,   320] loss: 0.001\n",
      "[1,   340] loss: 0.001\n",
      "[1,   360] loss: 0.001\n",
      "[1,   380] loss: 0.001\n",
      "[1,   400] loss: 0.001\n",
      "[1,   420] loss: 0.001\n",
      "[1,   440] loss: 0.001\n",
      "[1,   460] loss: 0.001\n",
      "[1,   480] loss: 0.001\n",
      "[1,   500] loss: 0.001\n",
      "[1,   520] loss: 0.001\n",
      "[1,   540] loss: 0.001\n",
      "[1,   560] loss: 0.001\n",
      "[1,   580] loss: 0.001\n",
      "[1,   600] loss: 0.001\n",
      "[1,   620] loss: 0.001\n",
      "[1,   640] loss: 0.001\n",
      "[1,   660] loss: 0.001\n",
      "[1,   680] loss: 0.001\n",
      "[1,   700] loss: 0.001\n",
      "[1,   720] loss: 0.001\n",
      "[1,   740] loss: 0.001\n",
      "[1,   760] loss: 0.001\n",
      "[1,   780] loss: 0.001\n",
      "[1,   800] loss: 0.001\n",
      "[1,   820] loss: 0.001\n",
      "[1,   840] loss: 0.001\n",
      "[1,   860] loss: 0.001\n",
      "[1,   880] loss: 0.001\n",
      "[1,   900] loss: 0.001\n",
      "[1,   920] loss: 0.001\n",
      "[2,    20] loss: 0.001\n",
      "[2,    40] loss: 0.001\n",
      "[2,    60] loss: 0.001\n",
      "[2,    80] loss: 0.001\n",
      "[2,   100] loss: 0.001\n",
      "[2,   120] loss: 0.001\n",
      "[2,   140] loss: 0.001\n",
      "[2,   160] loss: 0.001\n",
      "[2,   180] loss: 0.001\n",
      "[2,   200] loss: 0.001\n",
      "[2,   220] loss: 0.001\n",
      "[2,   240] loss: 0.001\n",
      "[2,   260] loss: 0.001\n",
      "[2,   280] loss: 0.001\n",
      "[2,   300] loss: 0.001\n",
      "[2,   320] loss: 0.001\n",
      "[2,   340] loss: 0.001\n",
      "[2,   360] loss: 0.001\n",
      "[2,   380] loss: 0.001\n",
      "[2,   400] loss: 0.001\n",
      "[2,   420] loss: 0.001\n",
      "[2,   440] loss: 0.001\n",
      "[2,   460] loss: 0.001\n",
      "[2,   480] loss: 0.001\n",
      "[2,   500] loss: 0.001\n",
      "[2,   520] loss: 0.001\n",
      "[2,   540] loss: 0.001\n",
      "[2,   560] loss: 0.001\n",
      "[2,   580] loss: 0.001\n",
      "[2,   600] loss: 0.001\n",
      "[2,   620] loss: 0.001\n",
      "[2,   640] loss: 0.001\n",
      "[2,   660] loss: 0.001\n",
      "[2,   680] loss: 0.001\n",
      "[2,   700] loss: 0.001\n",
      "[2,   720] loss: 0.001\n",
      "[2,   740] loss: 0.001\n",
      "[2,   760] loss: 0.001\n",
      "[2,   780] loss: 0.001\n",
      "[2,   800] loss: 0.001\n",
      "[2,   820] loss: 0.001\n",
      "[2,   840] loss: 0.001\n",
      "[2,   860] loss: 0.001\n",
      "[2,   880] loss: 0.001\n",
      "[2,   900] loss: 0.001\n",
      "[2,   920] loss: 0.001\n",
      "[3,    20] loss: 0.001\n",
      "[3,    40] loss: 0.001\n",
      "[3,    60] loss: 0.001\n",
      "[3,    80] loss: 0.001\n",
      "[3,   100] loss: 0.001\n",
      "[3,   120] loss: 0.001\n",
      "[3,   140] loss: 0.001\n",
      "[3,   160] loss: 0.001\n",
      "[3,   180] loss: 0.001\n",
      "[3,   200] loss: 0.001\n",
      "[3,   220] loss: 0.001\n",
      "[3,   240] loss: 0.001\n",
      "[3,   260] loss: 0.001\n",
      "[3,   280] loss: 0.001\n",
      "[3,   300] loss: 0.001\n",
      "[3,   320] loss: 0.001\n",
      "[3,   340] loss: 0.001\n",
      "[3,   360] loss: 0.001\n",
      "[3,   380] loss: 0.001\n",
      "[3,   400] loss: 0.001\n",
      "[3,   420] loss: 0.001\n",
      "[3,   440] loss: 0.001\n",
      "[3,   460] loss: 0.001\n",
      "[3,   480] loss: 0.001\n",
      "[3,   500] loss: 0.001\n",
      "[3,   520] loss: 0.001\n",
      "[3,   540] loss: 0.001\n",
      "[3,   560] loss: 0.001\n",
      "[3,   580] loss: 0.001\n",
      "[3,   600] loss: 0.001\n",
      "[3,   620] loss: 0.001\n",
      "[3,   640] loss: 0.001\n",
      "[3,   660] loss: 0.001\n",
      "[3,   680] loss: 0.001\n",
      "[3,   700] loss: 0.001\n",
      "[3,   720] loss: 0.001\n",
      "[3,   740] loss: 0.001\n",
      "[3,   760] loss: 0.001\n",
      "[3,   780] loss: 0.001\n",
      "[3,   800] loss: 0.001\n",
      "[3,   820] loss: 0.001\n",
      "[3,   840] loss: 0.001\n",
      "[3,   860] loss: 0.001\n",
      "[3,   880] loss: 0.001\n",
      "[3,   900] loss: 0.001\n",
      "[3,   920] loss: 0.001\n",
      "[4,    20] loss: 0.001\n",
      "[4,    40] loss: 0.001\n",
      "[4,    60] loss: 0.001\n",
      "[4,    80] loss: 0.001\n",
      "[4,   100] loss: 0.001\n",
      "[4,   120] loss: 0.001\n",
      "[4,   140] loss: 0.001\n",
      "[4,   160] loss: 0.001\n",
      "[4,   180] loss: 0.001\n",
      "[4,   200] loss: 0.001\n",
      "[4,   220] loss: 0.001\n",
      "[4,   240] loss: 0.001\n",
      "[4,   260] loss: 0.001\n",
      "[4,   280] loss: 0.001\n",
      "[4,   300] loss: 0.001\n",
      "[4,   320] loss: 0.001\n",
      "[4,   340] loss: 0.001\n",
      "[4,   360] loss: 0.001\n",
      "[4,   380] loss: 0.001\n",
      "[4,   400] loss: 0.001\n",
      "[4,   420] loss: 0.001\n",
      "[4,   440] loss: 0.001\n",
      "[4,   460] loss: 0.001\n",
      "[4,   480] loss: 0.001\n",
      "[4,   500] loss: 0.001\n",
      "[4,   520] loss: 0.001\n",
      "[4,   540] loss: 0.001\n",
      "[4,   560] loss: 0.001\n",
      "[4,   580] loss: 0.001\n",
      "[4,   600] loss: 0.001\n",
      "[4,   620] loss: 0.001\n",
      "[4,   640] loss: 0.001\n",
      "[4,   660] loss: 0.001\n",
      "[4,   680] loss: 0.001\n",
      "[4,   700] loss: 0.001\n",
      "[4,   720] loss: 0.001\n",
      "[4,   740] loss: 0.001\n",
      "[4,   760] loss: 0.001\n",
      "[4,   780] loss: 0.001\n",
      "[4,   800] loss: 0.001\n",
      "[4,   820] loss: 0.001\n",
      "[4,   840] loss: 0.001\n",
      "[4,   860] loss: 0.001\n",
      "[4,   880] loss: 0.001\n",
      "[4,   900] loss: 0.001\n",
      "[4,   920] loss: 0.001\n",
      "[5,    20] loss: 0.001\n",
      "[5,    40] loss: 0.001\n",
      "[5,    60] loss: 0.001\n",
      "[5,    80] loss: 0.001\n",
      "[5,   100] loss: 0.001\n",
      "[5,   120] loss: 0.001\n",
      "[5,   140] loss: 0.001\n",
      "[5,   160] loss: 0.001\n",
      "[5,   180] loss: 0.001\n",
      "[5,   200] loss: 0.001\n",
      "[5,   220] loss: 0.001\n",
      "[5,   240] loss: 0.001\n",
      "[5,   260] loss: 0.001\n",
      "[5,   280] loss: 0.001\n",
      "[5,   300] loss: 0.001\n",
      "[5,   320] loss: 0.001\n",
      "[5,   340] loss: 0.001\n",
      "[5,   360] loss: 0.001\n",
      "[5,   380] loss: 0.001\n",
      "[5,   400] loss: 0.001\n",
      "[5,   420] loss: 0.001\n",
      "[5,   440] loss: 0.001\n",
      "[5,   460] loss: 0.001\n",
      "[5,   480] loss: 0.001\n",
      "[5,   500] loss: 0.001\n",
      "[5,   520] loss: 0.001\n",
      "[5,   540] loss: 0.001\n",
      "[5,   560] loss: 0.001\n",
      "[5,   580] loss: 0.001\n",
      "[5,   600] loss: 0.001\n",
      "[5,   620] loss: 0.001\n",
      "[5,   640] loss: 0.001\n",
      "[5,   660] loss: 0.001\n",
      "[5,   680] loss: 0.001\n",
      "[5,   700] loss: 0.001\n",
      "[5,   720] loss: 0.001\n",
      "[5,   740] loss: 0.001\n",
      "[5,   760] loss: 0.001\n",
      "[5,   780] loss: 0.001\n",
      "[5,   800] loss: 0.001\n",
      "[5,   820] loss: 0.001\n",
      "[5,   840] loss: 0.001\n",
      "[5,   860] loss: 0.001\n",
      "[5,   880] loss: 0.001\n",
      "[5,   900] loss: 0.001\n",
      "[5,   920] loss: 0.001\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 95 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        _, labels =  torch.max(labels,1)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
